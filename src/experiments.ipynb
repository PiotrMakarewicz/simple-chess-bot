{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget -r -e robots=off -P ../ --no-parent  --no-host-directories --reject=\"index.html*\" --convert-links http://tablebase.lichess.ovh/tables/standard/3-4-5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from chess import syzygy\n",
    "from board.BoardEncoder import BoardEncoder\n",
    "import numpy as np\n",
    "from montecarlo.MonteCarloNode import MonteCarloNode\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tablebase = syzygy.Tablebase()\n",
    "tablebase.add_directory('../tables/standard/3-4-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing tablebase for selected positions\n",
    "\n",
    "We are using Syzygy tablebases. More information here: https://python-chess.readthedocs.io/en/latest/syzygy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dtz(fen):\n",
    "    dtz = tablebase.probe_dtz(chess.Board(fen))\n",
    "    print(f\"Distance to checkmate or a zeroing move: {dtz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/8/8/8/3k4/8/3K4/3Q4_w_-_-_0_1?color=white)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to checkmate or a zeroing move: 13\n"
     ]
    }
   ],
   "source": [
    "print_dtz('8/8/8/8/3k4/8/3K4/3Q4 w - - 0 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive value for DTZ indicates that the side to move needs 13 moves to win (or zero the 50-move counter), so here white is winning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/k7/p7/8/K7/8/8/8/8_w_-_-_0_1?color=white)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to checkmate or a zeroing move: 0\n"
     ]
    }
   ],
   "source": [
    "print_dtz('k7/p7/8/K7/8/8/8/8 w - - 0 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero means that the position is drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/R7/3r4/8/4K3/8/4p3/4k3%20w%20-%20-%200%201?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to checkmate or a zeroing move: -10\n"
     ]
    }
   ],
   "source": [
    "print_dtz('8/R7/3r4/8/4K3/8/4p3/4k3 w - - 0 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative value indicates that the side to play will lose in 10 moves, assuming the best play by both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtz_to_eval(dtz):\n",
    "    if dtz > 0:\n",
    "        return max(101 - dtz, 50)\n",
    "    elif dtz < 0:\n",
    "        return min(-101-dtz, -50)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitarray_to_ndarray(bitarray):\n",
    "    return np.array([int(bit) for bit in bitarray], dtype=bool)\n",
    "\n",
    "def put_piece(board, color, piece, check_validity=True):\n",
    "    while True:\n",
    "        square = np.random.randint(0, 64)\n",
    "        while board.piece_at(square) is not None:\n",
    "            square = np.random.randint(0, 64)\n",
    "        board.set_piece_at(square, chess.Piece(piece, color)) \n",
    "        if board.is_valid() or not check_validity:\n",
    "            break\n",
    "        else:\n",
    "            board.remove_piece_at(square)\n",
    "\n",
    "def generate_random_fen():\n",
    "    # Generate a random position, where\n",
    "    # for each side there is a king and optionally a queen and/or a rook and/or a pawn\n",
    "    \n",
    "    board = chess.Board()\n",
    "    board.clear_board()\n",
    "    board.castling_rights = 0\n",
    "    board.turn = chess.WHITE if np.random.randint(0, 2) == 0 else chess.BLACK\n",
    "\n",
    "    # Put kings first because positions without them are illegal\n",
    "    put_piece(board, chess.WHITE, chess.KING, check_validity=False)\n",
    "    put_piece(board, chess.BLACK, chess.KING, check_validity=True)\n",
    "\n",
    "    \n",
    "    color_piece_count = np.random.choice([\n",
    "        {chess.WHITE: {chess.QUEEN: 1,chess.PAWN: 0}, chess.BLACK: {chess.QUEEN: 0,chess.PAWN: 0}},\n",
    "        {chess.WHITE: {chess.QUEEN: 0,chess.PAWN: 1}, chess.BLACK: {chess.QUEEN: 0,chess.PAWN: 0}},\n",
    "        {chess.WHITE: {chess.QUEEN: 0,chess.PAWN: 0}, chess.BLACK: {chess.QUEEN: 1,chess.PAWN: 0}},\n",
    "        {chess.WHITE: {chess.QUEEN: 0,chess.PAWN: 0}, chess.BLACK: {chess.QUEEN: 0,chess.PAWN: 1}},\n",
    "        ])\n",
    "        \n",
    "\n",
    "    for color in chess.COLORS:\n",
    "        for piece, count in color_piece_count[color].items():\n",
    "            for _ in range(count):\n",
    "                put_piece(board, color, piece)\n",
    "    \n",
    "    return board.fen()\n",
    "                \n",
    "\n",
    "N_SAMPLES = 100000\n",
    "\n",
    "X_rows = []\n",
    "evals = []\n",
    "fens = []\n",
    "\n",
    "for _ in range(N_SAMPLES):\n",
    "    fen = generate_random_fen()\n",
    "    fens.append(fen)\n",
    "    \n",
    "    board = chess.Board(fen)\n",
    "    \n",
    "    X_row = bitarray_to_ndarray(BoardEncoder.encode(board))\n",
    "    X_rows.append(X_row)\n",
    "\n",
    "    dtz = tablebase.probe_dtz(board)\n",
    "    \n",
    "    evals.append(dtz_to_eval(dtz))\n",
    "\n",
    "X = np.array(X_rows)\n",
    "y = np.array(evals)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 837)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation to see if MLP regressor is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1703.32371643\n",
      "Iteration 2, loss = 488.80337755\n",
      "Iteration 3, loss = 445.01355708\n",
      "Iteration 4, loss = 403.88790431\n",
      "Iteration 5, loss = 364.41191236\n",
      "Iteration 6, loss = 328.12483349\n",
      "Iteration 7, loss = 294.21829638\n",
      "Iteration 8, loss = 264.11518751\n",
      "Iteration 9, loss = 239.06953842\n",
      "Iteration 10, loss = 218.10925252\n",
      "Iteration 11, loss = 202.72591118\n",
      "Iteration 12, loss = 187.90225959\n",
      "Iteration 13, loss = 175.23907353\n",
      "Iteration 14, loss = 163.68544802\n",
      "Iteration 15, loss = 154.45758432\n",
      "Iteration 16, loss = 145.19960749\n",
      "Iteration 17, loss = 136.67059087\n",
      "Iteration 18, loss = 129.47362318\n",
      "Iteration 19, loss = 121.63653178\n",
      "Iteration 20, loss = 113.95165940\n",
      "Iteration 21, loss = 107.63724418\n",
      "Iteration 22, loss = 100.10719278\n",
      "Iteration 23, loss = 95.15595879\n",
      "Iteration 24, loss = 89.23338265\n",
      "Iteration 25, loss = 85.31293287\n",
      "Iteration 26, loss = 79.75672074\n",
      "Iteration 27, loss = 76.62622311\n",
      "Iteration 28, loss = 72.38665520\n",
      "Iteration 29, loss = 68.51046139\n",
      "Iteration 30, loss = 65.67529797\n",
      "Iteration 31, loss = 62.92874966\n",
      "Iteration 32, loss = 61.27334126\n",
      "Iteration 33, loss = 58.30452543\n",
      "Iteration 34, loss = 55.95696529\n",
      "Iteration 35, loss = 54.31401270\n",
      "Iteration 36, loss = 54.50411886\n",
      "Iteration 37, loss = 50.92824662\n",
      "Iteration 38, loss = 50.04214758\n",
      "Iteration 39, loss = 48.99917766\n",
      "Iteration 40, loss = 48.04023438\n",
      "Iteration 41, loss = 47.12191124\n",
      "Iteration 42, loss = 45.84485256\n",
      "Iteration 43, loss = 44.41106547\n",
      "Iteration 44, loss = 43.80998206\n",
      "Iteration 45, loss = 43.01027182\n",
      "Iteration 46, loss = 41.40854473\n",
      "Iteration 47, loss = 40.86448613\n",
      "Iteration 48, loss = 40.51537731\n",
      "Iteration 49, loss = 39.44480157\n",
      "Iteration 50, loss = 38.60801221\n",
      "Iteration 51, loss = 38.58696562\n",
      "Iteration 52, loss = 36.86894388\n",
      "Iteration 53, loss = 36.26081338\n",
      "Iteration 54, loss = 36.41427481\n",
      "Iteration 55, loss = 35.24322357\n",
      "Iteration 56, loss = 34.72852982\n",
      "Iteration 57, loss = 34.56878605\n",
      "Iteration 58, loss = 33.85850497\n",
      "Iteration 59, loss = 33.35747512\n",
      "Iteration 60, loss = 32.73703894\n",
      "Iteration 61, loss = 31.94146142\n",
      "Iteration 62, loss = 32.30616176\n",
      "Iteration 63, loss = 30.90671723\n",
      "Iteration 64, loss = 30.51141921\n",
      "Iteration 65, loss = 30.89643649\n",
      "Iteration 66, loss = 30.88523571\n",
      "Iteration 67, loss = 29.99419257\n",
      "Iteration 68, loss = 29.65382936\n",
      "Iteration 69, loss = 29.36811489\n",
      "Iteration 70, loss = 28.27622946\n",
      "Iteration 71, loss = 28.15062202\n",
      "Iteration 72, loss = 28.63139921\n",
      "Iteration 73, loss = 28.39873850\n",
      "Iteration 74, loss = 27.23531730\n",
      "Iteration 75, loss = 27.36805739\n",
      "Iteration 76, loss = 26.56334447\n",
      "Iteration 77, loss = 26.13458859\n",
      "Iteration 78, loss = 26.24425595\n",
      "Iteration 79, loss = 25.96985721\n",
      "Iteration 80, loss = 25.56573662\n",
      "Iteration 81, loss = 25.78372243\n",
      "Iteration 82, loss = 25.52674709\n",
      "Iteration 83, loss = 25.22011024\n",
      "Iteration 84, loss = 24.01298271\n",
      "Iteration 85, loss = 23.66278976\n",
      "Iteration 86, loss = 23.71595699\n",
      "Iteration 87, loss = 23.92148162\n",
      "Iteration 88, loss = 23.20253737\n",
      "Iteration 89, loss = 23.30146955\n",
      "Iteration 90, loss = 23.67744479\n",
      "Iteration 91, loss = 22.95788591\n",
      "Iteration 92, loss = 22.77154973\n",
      "Iteration 93, loss = 22.75723981\n",
      "Iteration 94, loss = 21.94262344\n",
      "Iteration 95, loss = 21.89918994\n",
      "Iteration 96, loss = 22.20921336\n",
      "Iteration 97, loss = 22.42433552\n",
      "Iteration 98, loss = 21.59604106\n",
      "Iteration 99, loss = 21.85309916\n",
      "Iteration 100, loss = 21.22893858\n",
      "Iteration 101, loss = 21.06752321\n",
      "Iteration 102, loss = 20.71489776\n",
      "Iteration 103, loss = 20.27082167\n",
      "Iteration 104, loss = 20.65709836\n",
      "Iteration 105, loss = 20.60524239\n",
      "Iteration 106, loss = 20.48282898\n",
      "Iteration 107, loss = 20.17864395\n",
      "Iteration 108, loss = 20.09434129\n",
      "Iteration 109, loss = 20.14334107\n",
      "Iteration 110, loss = 19.58036386\n",
      "Iteration 111, loss = 19.24874778\n",
      "Iteration 112, loss = 19.32918481\n",
      "Iteration 113, loss = 19.54582870\n",
      "Iteration 114, loss = 19.60844777\n",
      "Iteration 115, loss = 18.80273533\n",
      "Iteration 116, loss = 19.23601459\n",
      "Iteration 117, loss = 18.68817647\n",
      "Iteration 118, loss = 18.99079936\n",
      "Iteration 119, loss = 18.56429752\n",
      "Iteration 120, loss = 17.79758325\n",
      "Iteration 121, loss = 18.88938673\n",
      "Iteration 122, loss = 19.10569529\n",
      "Iteration 123, loss = 17.80991780\n",
      "Iteration 124, loss = 17.74513680\n",
      "Iteration 125, loss = 17.44385238\n",
      "Iteration 126, loss = 17.89098662\n",
      "Iteration 127, loss = 17.41412672\n",
      "Iteration 128, loss = 17.59560307\n",
      "Iteration 129, loss = 17.38770232\n",
      "Iteration 130, loss = 17.95217438\n",
      "Iteration 131, loss = 17.60409048\n",
      "Iteration 132, loss = 17.12985737\n",
      "Iteration 133, loss = 17.38604777\n",
      "Iteration 134, loss = 16.99606168\n",
      "Iteration 135, loss = 17.22222807\n",
      "Iteration 136, loss = 17.04941429\n",
      "Iteration 137, loss = 17.14773234\n",
      "Iteration 138, loss = 16.57010641\n",
      "Iteration 139, loss = 16.26452826\n",
      "Iteration 140, loss = 16.07393532\n",
      "Iteration 141, loss = 17.06966944\n",
      "Iteration 142, loss = 16.13259269\n",
      "Iteration 143, loss = 15.83770374\n",
      "Iteration 144, loss = 16.11877725\n",
      "Iteration 145, loss = 16.30940132\n",
      "Iteration 146, loss = 16.19769379\n",
      "Iteration 147, loss = 15.93876740\n",
      "Iteration 148, loss = 15.69697013\n",
      "Iteration 149, loss = 16.36518635\n",
      "Iteration 150, loss = 15.81891041\n",
      "Iteration 151, loss = 16.41030818\n",
      "Iteration 152, loss = 15.75408813\n",
      "Iteration 153, loss = 16.00065695\n",
      "Iteration 154, loss = 15.46464353\n",
      "Iteration 155, loss = 15.38120648\n",
      "Iteration 156, loss = 15.29087798\n",
      "Iteration 157, loss = 15.37632498\n",
      "Iteration 158, loss = 15.29686556\n",
      "Iteration 159, loss = 15.21708635\n",
      "Iteration 160, loss = 15.50140585\n",
      "Iteration 161, loss = 15.53095817\n",
      "Iteration 162, loss = 15.18339804\n",
      "Iteration 163, loss = 15.41532488\n",
      "Iteration 164, loss = 15.16500409\n",
      "Iteration 165, loss = 15.04712535\n",
      "Iteration 166, loss = 14.76186738\n",
      "Iteration 167, loss = 14.32547039\n",
      "Iteration 168, loss = 14.95392034\n",
      "Iteration 169, loss = 14.61866938\n",
      "Iteration 170, loss = 15.10403356\n",
      "Iteration 171, loss = 15.05925288\n",
      "Iteration 172, loss = 14.57941943\n",
      "Iteration 173, loss = 14.08824448\n",
      "Iteration 174, loss = 15.03042802\n",
      "Iteration 175, loss = 14.81502208\n",
      "Iteration 176, loss = 14.83326941\n",
      "Iteration 177, loss = 14.22753360\n",
      "Iteration 178, loss = 14.19098834\n",
      "Iteration 179, loss = 14.11626176\n",
      "Iteration 180, loss = 14.43262923\n",
      "Iteration 181, loss = 15.08447425\n",
      "Iteration 182, loss = 14.18246304\n",
      "Iteration 183, loss = 13.88165942\n",
      "Iteration 184, loss = 14.17868602\n",
      "Iteration 185, loss = 13.94403033\n",
      "Iteration 186, loss = 14.52248701\n",
      "Iteration 187, loss = 14.07924838\n",
      "Iteration 188, loss = 13.97589372\n",
      "Iteration 189, loss = 13.76438649\n",
      "Iteration 190, loss = 13.89080803\n",
      "Iteration 191, loss = 14.31661453\n",
      "Iteration 192, loss = 13.72670762\n",
      "Iteration 193, loss = 13.69946300\n",
      "Iteration 194, loss = 14.04246994\n",
      "Iteration 195, loss = 13.95835933\n",
      "Iteration 196, loss = 13.93005439\n",
      "Iteration 197, loss = 13.95493588\n",
      "Iteration 198, loss = 13.91499265\n",
      "Iteration 199, loss = 13.46295446\n",
      "Iteration 200, loss = 13.70274835\n",
      "Iteration 201, loss = 13.29783846\n",
      "Iteration 202, loss = 13.14501965\n",
      "Iteration 203, loss = 13.05820551\n",
      "Iteration 204, loss = 13.53290809\n",
      "Iteration 205, loss = 14.45205105\n",
      "Iteration 206, loss = 13.89714059\n",
      "Iteration 207, loss = 13.42357756\n",
      "Iteration 208, loss = 13.62849007\n",
      "Iteration 209, loss = 12.86443195\n",
      "Iteration 210, loss = 12.91144660\n",
      "Iteration 211, loss = 13.58718361\n",
      "Iteration 212, loss = 13.20252999\n",
      "Iteration 213, loss = 13.46603996\n",
      "Iteration 214, loss = 13.70681934\n",
      "Iteration 215, loss = 12.96456411\n",
      "Iteration 216, loss = 12.61653653\n",
      "Iteration 217, loss = 13.07921239\n",
      "Iteration 218, loss = 12.97052312\n",
      "Iteration 219, loss = 13.40990095\n",
      "Iteration 220, loss = 13.00918971\n",
      "Iteration 221, loss = 12.70359041\n",
      "Iteration 222, loss = 12.76560215\n",
      "Iteration 223, loss = 13.02158198\n",
      "Iteration 224, loss = 12.55648837\n",
      "Iteration 225, loss = 13.21944958\n",
      "Iteration 226, loss = 13.00216057\n",
      "Iteration 227, loss = 12.81357015\n",
      "Iteration 228, loss = 12.64612824\n",
      "Iteration 229, loss = 12.60876777\n",
      "Iteration 230, loss = 12.76664861\n",
      "Iteration 231, loss = 12.52840830\n",
      "Iteration 232, loss = 12.91638882\n",
      "Iteration 233, loss = 12.89329208\n",
      "Iteration 234, loss = 12.99401564\n",
      "Iteration 235, loss = 12.80531032\n",
      "Iteration 236, loss = 12.60733098\n",
      "Iteration 237, loss = 12.22150875\n",
      "Iteration 238, loss = 12.08348437\n",
      "Iteration 239, loss = 12.47321955\n",
      "Iteration 240, loss = 12.80134378\n",
      "Iteration 241, loss = 12.69333534\n",
      "Iteration 242, loss = 12.99632627\n",
      "Iteration 243, loss = 12.66460647\n",
      "Iteration 244, loss = 12.09617084\n",
      "Iteration 245, loss = 11.92120520\n",
      "Iteration 246, loss = 12.18914281\n",
      "Iteration 247, loss = 12.70296279\n",
      "Iteration 248, loss = 12.87839765\n",
      "Iteration 249, loss = 12.66217220\n",
      "Iteration 250, loss = 12.83302431\n",
      "Iteration 251, loss = 12.10401447\n",
      "Iteration 252, loss = 11.95173182\n",
      "Iteration 253, loss = 12.26240327\n",
      "Iteration 254, loss = 12.52267608\n",
      "Iteration 255, loss = 11.95596298\n",
      "Iteration 256, loss = 12.35059855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1765.83472596\n",
      "Iteration 2, loss = 489.73483734\n",
      "Iteration 3, loss = 446.48695953\n",
      "Iteration 4, loss = 400.39472633\n",
      "Iteration 5, loss = 357.26249345\n",
      "Iteration 6, loss = 320.33222828\n",
      "Iteration 7, loss = 285.14891475\n",
      "Iteration 8, loss = 253.92993334\n",
      "Iteration 9, loss = 226.83398225\n",
      "Iteration 10, loss = 205.67304706\n",
      "Iteration 11, loss = 189.99434293\n",
      "Iteration 12, loss = 176.55515932\n",
      "Iteration 13, loss = 165.48075123\n",
      "Iteration 14, loss = 155.74320899\n",
      "Iteration 15, loss = 146.88675897\n",
      "Iteration 16, loss = 140.80588001\n",
      "Iteration 17, loss = 134.41276636\n",
      "Iteration 18, loss = 126.49517578\n",
      "Iteration 19, loss = 122.53947941\n",
      "Iteration 20, loss = 118.26478994\n",
      "Iteration 21, loss = 112.15592507\n",
      "Iteration 22, loss = 107.90157031\n",
      "Iteration 23, loss = 103.76052127\n",
      "Iteration 24, loss = 100.13355710\n",
      "Iteration 25, loss = 96.84068041\n",
      "Iteration 26, loss = 93.15272630\n",
      "Iteration 27, loss = 89.25369044\n",
      "Iteration 28, loss = 86.31163397\n",
      "Iteration 29, loss = 83.15051334\n",
      "Iteration 30, loss = 81.29012862\n",
      "Iteration 31, loss = 78.79393534\n",
      "Iteration 32, loss = 76.19161775\n",
      "Iteration 33, loss = 73.25311545\n",
      "Iteration 34, loss = 70.38935793\n",
      "Iteration 35, loss = 69.52884763\n",
      "Iteration 36, loss = 66.51112179\n",
      "Iteration 37, loss = 65.22081405\n",
      "Iteration 38, loss = 63.24780907\n",
      "Iteration 39, loss = 61.73736265\n",
      "Iteration 40, loss = 59.22981727\n",
      "Iteration 41, loss = 57.71039577\n",
      "Iteration 42, loss = 56.07081038\n",
      "Iteration 43, loss = 54.55430115\n",
      "Iteration 44, loss = 52.36302288\n",
      "Iteration 45, loss = 50.41265924\n",
      "Iteration 46, loss = 50.09181198\n",
      "Iteration 47, loss = 48.16347950\n",
      "Iteration 48, loss = 46.63092627\n",
      "Iteration 49, loss = 45.04758376\n",
      "Iteration 50, loss = 44.38283914\n",
      "Iteration 51, loss = 42.82101801\n",
      "Iteration 52, loss = 40.88637802\n",
      "Iteration 53, loss = 38.75960949\n",
      "Iteration 54, loss = 38.35010911\n",
      "Iteration 55, loss = 37.97859490\n",
      "Iteration 56, loss = 35.81221598\n",
      "Iteration 57, loss = 33.80730182\n",
      "Iteration 58, loss = 33.92279694\n",
      "Iteration 59, loss = 32.94846292\n",
      "Iteration 60, loss = 31.36439324\n",
      "Iteration 61, loss = 30.10669290\n",
      "Iteration 62, loss = 30.37984433\n",
      "Iteration 63, loss = 27.79490287\n",
      "Iteration 64, loss = 26.59962273\n",
      "Iteration 65, loss = 26.54557656\n",
      "Iteration 66, loss = 26.76528325\n",
      "Iteration 67, loss = 26.26231665\n",
      "Iteration 68, loss = 25.20861658\n",
      "Iteration 69, loss = 23.95368078\n",
      "Iteration 70, loss = 24.64244183\n",
      "Iteration 71, loss = 23.25558563\n",
      "Iteration 72, loss = 23.11055808\n",
      "Iteration 73, loss = 22.06256007\n",
      "Iteration 74, loss = 20.96845657\n",
      "Iteration 75, loss = 21.96108411\n",
      "Iteration 76, loss = 21.65660982\n",
      "Iteration 77, loss = 19.93003963\n",
      "Iteration 78, loss = 21.20725132\n",
      "Iteration 79, loss = 21.00332507\n",
      "Iteration 80, loss = 20.71446215\n",
      "Iteration 81, loss = 19.60458209\n",
      "Iteration 82, loss = 19.40170462\n",
      "Iteration 83, loss = 18.51337831\n",
      "Iteration 84, loss = 18.06534583\n",
      "Iteration 85, loss = 18.25552515\n",
      "Iteration 86, loss = 19.77579102\n",
      "Iteration 87, loss = 17.98365037\n",
      "Iteration 88, loss = 17.74515693\n",
      "Iteration 89, loss = 17.95202467\n",
      "Iteration 90, loss = 19.41419385\n",
      "Iteration 91, loss = 17.40914811\n",
      "Iteration 92, loss = 18.50367518\n",
      "Iteration 93, loss = 16.40326646\n",
      "Iteration 94, loss = 15.56207904\n",
      "Iteration 95, loss = 16.78845740\n",
      "Iteration 96, loss = 17.62534388\n",
      "Iteration 97, loss = 16.04831677\n",
      "Iteration 98, loss = 16.67130619\n",
      "Iteration 99, loss = 16.20468924\n",
      "Iteration 100, loss = 16.03364815\n",
      "Iteration 101, loss = 15.98905155\n",
      "Iteration 102, loss = 15.48440123\n",
      "Iteration 103, loss = 15.56575208\n",
      "Iteration 104, loss = 16.14812535\n",
      "Iteration 105, loss = 15.82716838\n",
      "Iteration 106, loss = 15.24507605\n",
      "Iteration 107, loss = 15.07238519\n",
      "Iteration 108, loss = 14.65596736\n",
      "Iteration 109, loss = 14.87824754\n",
      "Iteration 110, loss = 16.09277612\n",
      "Iteration 111, loss = 14.09259383\n",
      "Iteration 112, loss = 14.21318139\n",
      "Iteration 113, loss = 13.87820773\n",
      "Iteration 114, loss = 15.31329738\n",
      "Iteration 115, loss = 15.12289924\n",
      "Iteration 116, loss = 14.53128105\n",
      "Iteration 117, loss = 13.67390493\n",
      "Iteration 118, loss = 13.77412519\n",
      "Iteration 119, loss = 13.57203390\n",
      "Iteration 120, loss = 13.88491428\n",
      "Iteration 121, loss = 13.79412001\n",
      "Iteration 122, loss = 14.34414225\n",
      "Iteration 123, loss = 13.53796338\n",
      "Iteration 124, loss = 13.35314344\n",
      "Iteration 125, loss = 12.86964988\n",
      "Iteration 126, loss = 13.74757223\n",
      "Iteration 127, loss = 13.90945835\n",
      "Iteration 128, loss = 12.59834183\n",
      "Iteration 129, loss = 12.60403762\n",
      "Iteration 130, loss = 13.28409277\n",
      "Iteration 131, loss = 15.09435698\n",
      "Iteration 132, loss = 13.72851442\n",
      "Iteration 133, loss = 12.77104133\n",
      "Iteration 134, loss = 12.75175066\n",
      "Iteration 135, loss = 11.69559212\n",
      "Iteration 136, loss = 11.73316475\n",
      "Iteration 137, loss = 11.83492834\n",
      "Iteration 138, loss = 12.37663649\n",
      "Iteration 139, loss = 12.99685772\n",
      "Iteration 140, loss = 13.47909627\n",
      "Iteration 141, loss = 12.30677254\n",
      "Iteration 142, loss = 12.54454904\n",
      "Iteration 143, loss = 11.24799529\n",
      "Iteration 144, loss = 11.47974769\n",
      "Iteration 145, loss = 12.04548824\n",
      "Iteration 146, loss = 12.41751883\n",
      "Iteration 147, loss = 12.63528848\n",
      "Iteration 148, loss = 12.39567628\n",
      "Iteration 149, loss = 10.98302940\n",
      "Iteration 150, loss = 11.82421722\n",
      "Iteration 151, loss = 11.42015039\n",
      "Iteration 152, loss = 11.37101931\n",
      "Iteration 153, loss = 13.55982209\n",
      "Iteration 154, loss = 11.34072523\n",
      "Iteration 155, loss = 10.98511717\n",
      "Iteration 156, loss = 10.43411906\n",
      "Iteration 157, loss = 11.62460383\n",
      "Iteration 158, loss = 11.54514343\n",
      "Iteration 159, loss = 11.31426469\n",
      "Iteration 160, loss = 11.45613299\n",
      "Iteration 161, loss = 10.67711747\n",
      "Iteration 162, loss = 10.79193156\n",
      "Iteration 163, loss = 10.67573322\n",
      "Iteration 164, loss = 12.63263095\n",
      "Iteration 165, loss = 11.78864078\n",
      "Iteration 166, loss = 10.43637246\n",
      "Iteration 167, loss = 9.76918800\n",
      "Iteration 168, loss = 10.01975754\n",
      "Iteration 169, loss = 11.19742903\n",
      "Iteration 170, loss = 10.49051261\n",
      "Iteration 171, loss = 10.94478041\n",
      "Iteration 172, loss = 10.84243920\n",
      "Iteration 173, loss = 11.35667964\n",
      "Iteration 174, loss = 10.92464498\n",
      "Iteration 175, loss = 9.81541222\n",
      "Iteration 176, loss = 10.11991610\n",
      "Iteration 177, loss = 10.95462622\n",
      "Iteration 178, loss = 9.74624395\n",
      "Iteration 179, loss = 10.41725235\n",
      "Iteration 180, loss = 10.09146889\n",
      "Iteration 181, loss = 11.82862516\n",
      "Iteration 182, loss = 10.20271171\n",
      "Iteration 183, loss = 9.91258214\n",
      "Iteration 184, loss = 9.78678630\n",
      "Iteration 185, loss = 9.39665294\n",
      "Iteration 186, loss = 9.35075618\n",
      "Iteration 187, loss = 9.32736367\n",
      "Iteration 188, loss = 11.27593842\n",
      "Iteration 189, loss = 12.33877886\n",
      "Iteration 190, loss = 10.02833110\n",
      "Iteration 191, loss = 8.40311855\n",
      "Iteration 192, loss = 8.96835642\n",
      "Iteration 193, loss = 10.43725007\n",
      "Iteration 194, loss = 10.25120762\n",
      "Iteration 195, loss = 10.04418942\n",
      "Iteration 196, loss = 9.87481115\n",
      "Iteration 197, loss = 10.21805805\n",
      "Iteration 198, loss = 8.73409954\n",
      "Iteration 199, loss = 9.29391025\n",
      "Iteration 200, loss = 10.02724310\n",
      "Iteration 201, loss = 10.29187582\n",
      "Iteration 202, loss = 9.71949096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1775.33424416\n",
      "Iteration 2, loss = 489.18838915\n",
      "Iteration 3, loss = 437.03575195\n",
      "Iteration 4, loss = 389.19374419\n",
      "Iteration 5, loss = 339.91695026\n",
      "Iteration 6, loss = 297.43261356\n",
      "Iteration 7, loss = 265.73385332\n",
      "Iteration 8, loss = 239.28275547\n",
      "Iteration 9, loss = 216.44981321\n",
      "Iteration 10, loss = 201.81499891\n",
      "Iteration 11, loss = 185.80561487\n",
      "Iteration 12, loss = 173.42433273\n",
      "Iteration 13, loss = 163.73599984\n",
      "Iteration 14, loss = 155.16272017\n",
      "Iteration 15, loss = 146.88768982\n",
      "Iteration 16, loss = 138.73103114\n",
      "Iteration 17, loss = 131.61764886\n",
      "Iteration 18, loss = 124.96754818\n",
      "Iteration 19, loss = 120.37030238\n",
      "Iteration 20, loss = 114.57195905\n",
      "Iteration 21, loss = 110.79356261\n",
      "Iteration 22, loss = 106.12228875\n",
      "Iteration 23, loss = 102.69530643\n",
      "Iteration 24, loss = 98.07368301\n",
      "Iteration 25, loss = 95.79074251\n",
      "Iteration 26, loss = 93.02520882\n",
      "Iteration 27, loss = 89.20972695\n",
      "Iteration 28, loss = 86.82010266\n",
      "Iteration 29, loss = 83.89039394\n",
      "Iteration 30, loss = 81.07081020\n",
      "Iteration 31, loss = 79.76258671\n",
      "Iteration 32, loss = 76.66670679\n",
      "Iteration 33, loss = 74.97777633\n",
      "Iteration 34, loss = 72.34212328\n",
      "Iteration 35, loss = 70.57987624\n",
      "Iteration 36, loss = 68.39287418\n",
      "Iteration 37, loss = 65.07281462\n",
      "Iteration 38, loss = 63.59412274\n",
      "Iteration 39, loss = 61.07594034\n",
      "Iteration 40, loss = 59.11832015\n",
      "Iteration 41, loss = 57.26528431\n",
      "Iteration 42, loss = 54.38726139\n",
      "Iteration 43, loss = 52.32523018\n",
      "Iteration 44, loss = 50.71109305\n",
      "Iteration 45, loss = 49.06410194\n",
      "Iteration 46, loss = 46.50625240\n",
      "Iteration 47, loss = 46.02480174\n",
      "Iteration 48, loss = 43.87169318\n",
      "Iteration 49, loss = 42.63235339\n",
      "Iteration 50, loss = 42.40291641\n",
      "Iteration 51, loss = 40.21352106\n",
      "Iteration 52, loss = 37.94088789\n",
      "Iteration 53, loss = 36.73521484\n",
      "Iteration 54, loss = 36.64265895\n",
      "Iteration 55, loss = 35.71448700\n",
      "Iteration 56, loss = 34.24554286\n",
      "Iteration 57, loss = 33.42649179\n",
      "Iteration 58, loss = 31.89973947\n",
      "Iteration 59, loss = 31.35425174\n",
      "Iteration 60, loss = 31.17208728\n",
      "Iteration 61, loss = 31.46037116\n",
      "Iteration 62, loss = 28.98990330\n",
      "Iteration 63, loss = 28.19709027\n",
      "Iteration 64, loss = 29.62341120\n",
      "Iteration 65, loss = 27.48726335\n",
      "Iteration 66, loss = 24.77888242\n",
      "Iteration 67, loss = 25.59496671\n",
      "Iteration 68, loss = 27.22061258\n",
      "Iteration 69, loss = 26.00343206\n",
      "Iteration 70, loss = 24.49712055\n",
      "Iteration 71, loss = 23.85378008\n",
      "Iteration 72, loss = 23.71515507\n",
      "Iteration 73, loss = 20.97743914\n",
      "Iteration 74, loss = 21.88110167\n",
      "Iteration 75, loss = 22.31024983\n",
      "Iteration 76, loss = 21.75515081\n",
      "Iteration 77, loss = 18.76443309\n",
      "Iteration 78, loss = 17.67818317\n",
      "Iteration 79, loss = 17.04896317\n",
      "Iteration 80, loss = 19.05459198\n",
      "Iteration 81, loss = 19.77802588\n",
      "Iteration 82, loss = 17.46703202\n",
      "Iteration 83, loss = 17.63455229\n",
      "Iteration 84, loss = 17.03945787\n",
      "Iteration 85, loss = 16.23709369\n",
      "Iteration 86, loss = 16.18930147\n",
      "Iteration 87, loss = 15.09889229\n",
      "Iteration 88, loss = 14.43113857\n",
      "Iteration 89, loss = 14.50415461\n",
      "Iteration 90, loss = 14.62387250\n",
      "Iteration 91, loss = 15.04074855\n",
      "Iteration 92, loss = 16.06796120\n",
      "Iteration 93, loss = 14.45917014\n",
      "Iteration 94, loss = 14.39778204\n",
      "Iteration 95, loss = 15.02486459\n",
      "Iteration 96, loss = 13.51559656\n",
      "Iteration 97, loss = 12.84818935\n",
      "Iteration 98, loss = 13.53596448\n",
      "Iteration 99, loss = 13.09356185\n",
      "Iteration 100, loss = 12.06270351\n",
      "Iteration 101, loss = 12.35337352\n",
      "Iteration 102, loss = 13.32827407\n",
      "Iteration 103, loss = 13.86427589\n",
      "Iteration 104, loss = 13.52711205\n",
      "Iteration 105, loss = 13.56809660\n",
      "Iteration 106, loss = 11.97912364\n",
      "Iteration 107, loss = 12.43942224\n",
      "Iteration 108, loss = 12.29568349\n",
      "Iteration 109, loss = 11.90990016\n",
      "Iteration 110, loss = 11.48442238\n",
      "Iteration 111, loss = 10.93837933\n",
      "Iteration 112, loss = 13.74702226\n",
      "Iteration 113, loss = 12.77093399\n",
      "Iteration 114, loss = 12.06612632\n",
      "Iteration 115, loss = 10.99706629\n",
      "Iteration 116, loss = 11.52490470\n",
      "Iteration 117, loss = 11.98827131\n",
      "Iteration 118, loss = 12.34556059\n",
      "Iteration 119, loss = 11.96569384\n",
      "Iteration 120, loss = 12.02175434\n",
      "Iteration 121, loss = 11.48019316\n",
      "Iteration 122, loss = 10.89243628\n",
      "Iteration 123, loss = 11.05760614\n",
      "Iteration 124, loss = 13.75781013\n",
      "Iteration 125, loss = 11.92220671\n",
      "Iteration 126, loss = 9.41347313\n",
      "Iteration 127, loss = 8.63222066\n",
      "Iteration 128, loss = 8.17201260\n",
      "Iteration 129, loss = 10.61599344\n",
      "Iteration 130, loss = 15.86490441\n",
      "Iteration 131, loss = 12.31907302\n",
      "Iteration 132, loss = 10.98308263\n",
      "Iteration 133, loss = 10.32054595\n",
      "Iteration 134, loss = 10.34579842\n",
      "Iteration 135, loss = 10.99243202\n",
      "Iteration 136, loss = 9.78952358\n",
      "Iteration 137, loss = 10.23609049\n",
      "Iteration 138, loss = 12.31575401\n",
      "Iteration 139, loss = 12.17456746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1741.76304236\n",
      "Iteration 2, loss = 491.85157522\n",
      "Iteration 3, loss = 445.42976256\n",
      "Iteration 4, loss = 396.31318763\n",
      "Iteration 5, loss = 345.87054191\n",
      "Iteration 6, loss = 302.03041819\n",
      "Iteration 7, loss = 267.76738869\n",
      "Iteration 8, loss = 239.66770429\n",
      "Iteration 9, loss = 217.69422706\n",
      "Iteration 10, loss = 198.66374059\n",
      "Iteration 11, loss = 183.93073497\n",
      "Iteration 12, loss = 172.49616835\n",
      "Iteration 13, loss = 161.72415059\n",
      "Iteration 14, loss = 152.31750730\n",
      "Iteration 15, loss = 144.11452740\n",
      "Iteration 16, loss = 137.40407655\n",
      "Iteration 17, loss = 131.30722906\n",
      "Iteration 18, loss = 125.77081561\n",
      "Iteration 19, loss = 120.69389592\n",
      "Iteration 20, loss = 116.15918371\n",
      "Iteration 21, loss = 111.68401317\n",
      "Iteration 22, loss = 108.79182409\n",
      "Iteration 23, loss = 104.59377875\n",
      "Iteration 24, loss = 102.24118074\n",
      "Iteration 25, loss = 97.64798608\n",
      "Iteration 26, loss = 96.00816406\n",
      "Iteration 27, loss = 93.31547923\n",
      "Iteration 28, loss = 90.33825323\n",
      "Iteration 29, loss = 87.20989947\n",
      "Iteration 30, loss = 84.50621751\n",
      "Iteration 31, loss = 82.03950219\n",
      "Iteration 32, loss = 80.13009274\n",
      "Iteration 33, loss = 78.15955021\n",
      "Iteration 34, loss = 76.35679805\n",
      "Iteration 35, loss = 75.68381681\n",
      "Iteration 36, loss = 72.79224557\n",
      "Iteration 37, loss = 71.53531820\n",
      "Iteration 38, loss = 69.54767893\n",
      "Iteration 39, loss = 68.25980587\n",
      "Iteration 40, loss = 66.59715581\n",
      "Iteration 41, loss = 64.62453543\n",
      "Iteration 42, loss = 63.90324534\n",
      "Iteration 43, loss = 62.36905692\n",
      "Iteration 44, loss = 61.42118697\n",
      "Iteration 45, loss = 59.33048096\n",
      "Iteration 46, loss = 59.53580207\n",
      "Iteration 47, loss = 57.30956154\n",
      "Iteration 48, loss = 56.46135720\n",
      "Iteration 49, loss = 56.38571306\n",
      "Iteration 50, loss = 54.43373843\n",
      "Iteration 51, loss = 53.01242455\n",
      "Iteration 52, loss = 52.90414060\n",
      "Iteration 53, loss = 52.32764356\n",
      "Iteration 54, loss = 51.48038853\n",
      "Iteration 55, loss = 50.66566288\n",
      "Iteration 56, loss = 49.14793943\n",
      "Iteration 57, loss = 48.80852215\n",
      "Iteration 58, loss = 47.82782510\n",
      "Iteration 59, loss = 47.45510172\n",
      "Iteration 60, loss = 46.83417732\n",
      "Iteration 61, loss = 46.10015584\n",
      "Iteration 62, loss = 45.08661026\n",
      "Iteration 63, loss = 45.34915875\n",
      "Iteration 64, loss = 43.44567059\n",
      "Iteration 65, loss = 42.88689647\n",
      "Iteration 66, loss = 41.80833672\n",
      "Iteration 67, loss = 42.05431350\n",
      "Iteration 68, loss = 39.78130238\n",
      "Iteration 69, loss = 40.10342576\n",
      "Iteration 70, loss = 40.01543996\n",
      "Iteration 71, loss = 39.80299597\n",
      "Iteration 72, loss = 37.54065239\n",
      "Iteration 73, loss = 37.14793274\n",
      "Iteration 74, loss = 37.99203746\n",
      "Iteration 75, loss = 36.46882311\n",
      "Iteration 76, loss = 36.00882053\n",
      "Iteration 77, loss = 35.12968806\n",
      "Iteration 78, loss = 35.17486616\n",
      "Iteration 79, loss = 34.25687689\n",
      "Iteration 80, loss = 35.28931388\n",
      "Iteration 81, loss = 34.85398054\n",
      "Iteration 82, loss = 33.27368684\n",
      "Iteration 83, loss = 30.57482570\n",
      "Iteration 84, loss = 29.98852657\n",
      "Iteration 85, loss = 31.61454582\n",
      "Iteration 86, loss = 32.84644181\n",
      "Iteration 87, loss = 30.88000499\n",
      "Iteration 88, loss = 28.70545049\n",
      "Iteration 89, loss = 28.52109623\n",
      "Iteration 90, loss = 29.47762780\n",
      "Iteration 91, loss = 28.77169221\n",
      "Iteration 92, loss = 27.93728688\n",
      "Iteration 93, loss = 28.42132456\n",
      "Iteration 94, loss = 25.72873384\n",
      "Iteration 95, loss = 24.82329878\n",
      "Iteration 96, loss = 26.37609302\n",
      "Iteration 97, loss = 28.80439063\n",
      "Iteration 98, loss = 25.25623265\n",
      "Iteration 99, loss = 24.44988506\n",
      "Iteration 100, loss = 23.21943379\n",
      "Iteration 101, loss = 21.69862498\n",
      "Iteration 102, loss = 22.38310491\n",
      "Iteration 103, loss = 22.69180286\n",
      "Iteration 104, loss = 23.89774942\n",
      "Iteration 105, loss = 22.14411221\n",
      "Iteration 106, loss = 21.61650179\n",
      "Iteration 107, loss = 21.49469016\n",
      "Iteration 108, loss = 19.15121724\n",
      "Iteration 109, loss = 18.29483441\n",
      "Iteration 110, loss = 18.90006021\n",
      "Iteration 111, loss = 22.26243752\n",
      "Iteration 112, loss = 17.60821542\n",
      "Iteration 113, loss = 17.63680015\n",
      "Iteration 114, loss = 17.09997545\n",
      "Iteration 115, loss = 19.31844440\n",
      "Iteration 116, loss = 19.61716785\n",
      "Iteration 117, loss = 14.97996321\n",
      "Iteration 118, loss = 15.85302758\n",
      "Iteration 119, loss = 15.49563327\n",
      "Iteration 120, loss = 14.79430158\n",
      "Iteration 121, loss = 17.36955358\n",
      "Iteration 122, loss = 15.03145572\n",
      "Iteration 123, loss = 15.24723016\n",
      "Iteration 124, loss = 14.06279568\n",
      "Iteration 125, loss = 15.17266425\n",
      "Iteration 126, loss = 15.58570007\n",
      "Iteration 127, loss = 15.74445223\n",
      "Iteration 128, loss = 13.76766313\n",
      "Iteration 129, loss = 13.71370170\n",
      "Iteration 130, loss = 14.47590680\n",
      "Iteration 131, loss = 12.58440582\n",
      "Iteration 132, loss = 13.86967839\n",
      "Iteration 133, loss = 12.49159271\n",
      "Iteration 134, loss = 12.98150280\n",
      "Iteration 135, loss = 15.07203880\n",
      "Iteration 136, loss = 14.49219111\n",
      "Iteration 137, loss = 13.31546406\n",
      "Iteration 138, loss = 11.98681020\n",
      "Iteration 139, loss = 12.81224147\n",
      "Iteration 140, loss = 12.71639621\n",
      "Iteration 141, loss = 13.07074968\n",
      "Iteration 142, loss = 12.65501382\n",
      "Iteration 143, loss = 12.05056407\n",
      "Iteration 144, loss = 12.14091845\n",
      "Iteration 145, loss = 12.65513375\n",
      "Iteration 146, loss = 14.01346787\n",
      "Iteration 147, loss = 12.56727026\n",
      "Iteration 148, loss = 11.31362930\n",
      "Iteration 149, loss = 10.24848539\n",
      "Iteration 150, loss = 12.09066300\n",
      "Iteration 151, loss = 13.21852677\n",
      "Iteration 152, loss = 13.86545146\n",
      "Iteration 153, loss = 10.57508723\n",
      "Iteration 154, loss = 10.66927668\n",
      "Iteration 155, loss = 11.72013413\n",
      "Iteration 156, loss = 13.09032958\n",
      "Iteration 157, loss = 12.61181448\n",
      "Iteration 158, loss = 11.39481878\n",
      "Iteration 159, loss = 10.98296062\n",
      "Iteration 160, loss = 10.62315564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1750.86352224\n",
      "Iteration 2, loss = 482.19357331\n",
      "Iteration 3, loss = 436.32068322\n",
      "Iteration 4, loss = 392.30644505\n",
      "Iteration 5, loss = 343.27059518\n",
      "Iteration 6, loss = 298.59552777\n",
      "Iteration 7, loss = 265.05310505\n",
      "Iteration 8, loss = 237.70058857\n",
      "Iteration 9, loss = 216.28187927\n",
      "Iteration 10, loss = 199.57286509\n",
      "Iteration 11, loss = 184.73810743\n",
      "Iteration 12, loss = 171.86408064\n",
      "Iteration 13, loss = 160.04761859\n",
      "Iteration 14, loss = 151.18948921\n",
      "Iteration 15, loss = 143.40408961\n",
      "Iteration 16, loss = 135.70946914\n",
      "Iteration 17, loss = 128.96444829\n",
      "Iteration 18, loss = 123.20483160\n",
      "Iteration 19, loss = 118.02712050\n",
      "Iteration 20, loss = 112.44768324\n",
      "Iteration 21, loss = 108.82490357\n",
      "Iteration 22, loss = 103.88487916\n",
      "Iteration 23, loss = 98.03419516\n",
      "Iteration 24, loss = 94.77597154\n",
      "Iteration 25, loss = 91.13028193\n",
      "Iteration 26, loss = 87.84890824\n",
      "Iteration 27, loss = 83.24569494\n",
      "Iteration 28, loss = 80.13435107\n",
      "Iteration 29, loss = 75.99367104\n",
      "Iteration 30, loss = 72.79569709\n",
      "Iteration 31, loss = 70.63353265\n",
      "Iteration 32, loss = 67.97625954\n",
      "Iteration 33, loss = 65.88838690\n",
      "Iteration 34, loss = 63.26016940\n",
      "Iteration 35, loss = 60.46650936\n",
      "Iteration 36, loss = 58.07774042\n",
      "Iteration 37, loss = 55.85350080\n",
      "Iteration 38, loss = 53.64790965\n",
      "Iteration 39, loss = 51.29205152\n",
      "Iteration 40, loss = 50.17324435\n",
      "Iteration 41, loss = 46.77135638\n",
      "Iteration 42, loss = 45.16341214\n",
      "Iteration 43, loss = 41.94524900\n",
      "Iteration 44, loss = 40.14981465\n",
      "Iteration 45, loss = 38.00486693\n",
      "Iteration 46, loss = 36.67653054\n",
      "Iteration 47, loss = 35.24228771\n",
      "Iteration 48, loss = 33.24645292\n",
      "Iteration 49, loss = 32.64432087\n",
      "Iteration 50, loss = 30.56522517\n",
      "Iteration 51, loss = 28.55138124\n",
      "Iteration 52, loss = 28.75242261\n",
      "Iteration 53, loss = 27.21662712\n",
      "Iteration 54, loss = 26.11073143\n",
      "Iteration 55, loss = 24.38627981\n",
      "Iteration 56, loss = 24.08078327\n",
      "Iteration 57, loss = 22.36743600\n",
      "Iteration 58, loss = 21.77263255\n",
      "Iteration 59, loss = 21.94312569\n",
      "Iteration 60, loss = 19.46289941\n",
      "Iteration 61, loss = 19.23683168\n",
      "Iteration 62, loss = 19.89084124\n",
      "Iteration 63, loss = 19.25695621\n",
      "Iteration 64, loss = 17.75071639\n",
      "Iteration 65, loss = 15.89303376\n",
      "Iteration 66, loss = 16.25930947\n",
      "Iteration 67, loss = 16.68911077\n",
      "Iteration 68, loss = 16.92159776\n",
      "Iteration 69, loss = 15.99192948\n",
      "Iteration 70, loss = 13.99586546\n",
      "Iteration 71, loss = 13.75814013\n",
      "Iteration 72, loss = 14.05283222\n",
      "Iteration 73, loss = 14.88039872\n",
      "Iteration 74, loss = 16.30541912\n",
      "Iteration 75, loss = 13.53596401\n",
      "Iteration 76, loss = 12.89574594\n",
      "Iteration 77, loss = 12.94287396\n",
      "Iteration 78, loss = 13.04670411\n",
      "Iteration 79, loss = 14.16453490\n",
      "Iteration 80, loss = 13.90002351\n",
      "Iteration 81, loss = 12.34479333\n",
      "Iteration 82, loss = 13.14649840\n",
      "Iteration 83, loss = 12.26921431\n",
      "Iteration 84, loss = 12.57149260\n",
      "Iteration 85, loss = 11.52291452\n",
      "Iteration 86, loss = 10.68876422\n",
      "Iteration 87, loss = 14.55994930\n",
      "Iteration 88, loss = 13.76568169\n",
      "Iteration 89, loss = 12.30516576\n",
      "Iteration 90, loss = 11.31348687\n",
      "Iteration 91, loss = 10.14711874\n",
      "Iteration 92, loss = 12.00169629\n",
      "Iteration 93, loss = 13.33824965\n",
      "Iteration 94, loss = 11.63867786\n",
      "Iteration 95, loss = 10.64130462\n",
      "Iteration 96, loss = 12.10370209\n",
      "Iteration 97, loss = 11.81896009\n",
      "Iteration 98, loss = 12.20681799\n",
      "Iteration 99, loss = 10.99108474\n",
      "Iteration 100, loss = 10.17450384\n",
      "Iteration 101, loss = 12.15479117\n",
      "Iteration 102, loss = 11.64765552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Crossvalidation scores: [-9.45095992 -6.62258989 -6.37993635 -6.66929661 -6.61030222]\n",
      "Mean: -7.146616996162871\n",
      "Std: 1.1565351892731404\n"
     ]
    }
   ],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=1000, verbose=True, alpha=0.5)\n",
    "\n",
    "cv_score = cross_val_score(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(f\"Crossvalidation scores: {cv_score}\")\n",
    "print(f\"Mean: {cv_score.mean()}\")\n",
    "print(f\"Std: {cv_score.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1584.28725256\n",
      "Iteration 2, loss = 481.95163780\n",
      "Iteration 3, loss = 435.38136582\n",
      "Iteration 4, loss = 387.38263376\n",
      "Iteration 5, loss = 338.71017664\n",
      "Iteration 6, loss = 298.88588138\n",
      "Iteration 7, loss = 266.14195645\n",
      "Iteration 8, loss = 241.53696283\n",
      "Iteration 9, loss = 220.24892459\n",
      "Iteration 10, loss = 203.30218328\n",
      "Iteration 11, loss = 189.25050072\n",
      "Iteration 12, loss = 179.75293424\n",
      "Iteration 13, loss = 169.19430729\n",
      "Iteration 14, loss = 161.05475306\n",
      "Iteration 15, loss = 152.16622192\n",
      "Iteration 16, loss = 147.69550470\n",
      "Iteration 17, loss = 140.86009110\n",
      "Iteration 18, loss = 134.47376824\n",
      "Iteration 19, loss = 129.50207721\n",
      "Iteration 20, loss = 125.66232566\n",
      "Iteration 21, loss = 120.66184832\n",
      "Iteration 22, loss = 117.35042268\n",
      "Iteration 23, loss = 114.06602498\n",
      "Iteration 24, loss = 111.51779102\n",
      "Iteration 25, loss = 108.38700606\n",
      "Iteration 26, loss = 104.55923374\n",
      "Iteration 27, loss = 102.00537090\n",
      "Iteration 28, loss = 99.74392155\n",
      "Iteration 29, loss = 98.25715623\n",
      "Iteration 30, loss = 95.75223455\n",
      "Iteration 31, loss = 95.46046403\n",
      "Iteration 32, loss = 91.24406944\n",
      "Iteration 33, loss = 89.42168706\n",
      "Iteration 34, loss = 89.43877687\n",
      "Iteration 35, loss = 86.77163703\n",
      "Iteration 36, loss = 85.22836317\n",
      "Iteration 37, loss = 84.84635868\n",
      "Iteration 38, loss = 83.38561189\n",
      "Iteration 39, loss = 81.16572103\n",
      "Iteration 40, loss = 80.75587472\n",
      "Iteration 41, loss = 78.40913656\n",
      "Iteration 42, loss = 78.46003741\n",
      "Iteration 43, loss = 77.29347768\n",
      "Iteration 44, loss = 75.12968671\n",
      "Iteration 45, loss = 75.70443796\n",
      "Iteration 46, loss = 73.50168190\n",
      "Iteration 47, loss = 73.02063890\n",
      "Iteration 48, loss = 71.90017306\n",
      "Iteration 49, loss = 72.08711170\n",
      "Iteration 50, loss = 70.69722078\n",
      "Iteration 51, loss = 69.19532143\n",
      "Iteration 52, loss = 69.00684878\n",
      "Iteration 53, loss = 67.52886255\n",
      "Iteration 54, loss = 67.65550350\n",
      "Iteration 55, loss = 67.26731750\n",
      "Iteration 56, loss = 65.93422119\n",
      "Iteration 57, loss = 64.43006063\n",
      "Iteration 58, loss = 63.76730677\n",
      "Iteration 59, loss = 64.12372893\n",
      "Iteration 60, loss = 62.69017968\n",
      "Iteration 61, loss = 62.44631072\n",
      "Iteration 62, loss = 61.71267355\n",
      "Iteration 63, loss = 61.16740384\n",
      "Iteration 64, loss = 60.65582901\n",
      "Iteration 65, loss = 59.81781781\n",
      "Iteration 66, loss = 59.36934518\n",
      "Iteration 67, loss = 58.52362062\n",
      "Iteration 68, loss = 58.56493308\n",
      "Iteration 69, loss = 57.55276899\n",
      "Iteration 70, loss = 56.68458526\n",
      "Iteration 71, loss = 56.97489288\n",
      "Iteration 72, loss = 55.31413026\n",
      "Iteration 73, loss = 54.72067038\n",
      "Iteration 74, loss = 54.68676390\n",
      "Iteration 75, loss = 54.45749077\n",
      "Iteration 76, loss = 53.25031872\n",
      "Iteration 77, loss = 53.31368222\n",
      "Iteration 78, loss = 52.16544438\n",
      "Iteration 79, loss = 52.21979340\n",
      "Iteration 80, loss = 52.25713579\n",
      "Iteration 81, loss = 51.35754438\n",
      "Iteration 82, loss = 50.79560833\n",
      "Iteration 83, loss = 50.16893912\n",
      "Iteration 84, loss = 49.88129685\n",
      "Iteration 85, loss = 48.49430725\n",
      "Iteration 86, loss = 48.96166863\n",
      "Iteration 87, loss = 48.70769872\n",
      "Iteration 88, loss = 48.08542571\n",
      "Iteration 89, loss = 47.75508164\n",
      "Iteration 90, loss = 47.73842383\n",
      "Iteration 91, loss = 46.98603254\n",
      "Iteration 92, loss = 46.91701681\n",
      "Iteration 93, loss = 45.91518780\n",
      "Iteration 94, loss = 46.11081288\n",
      "Iteration 95, loss = 45.94216042\n",
      "Iteration 96, loss = 44.96716016\n",
      "Iteration 97, loss = 44.87111494\n",
      "Iteration 98, loss = 44.67113888\n",
      "Iteration 99, loss = 43.79511850\n",
      "Iteration 100, loss = 43.60834106\n",
      "Iteration 101, loss = 43.22794958\n",
      "Iteration 102, loss = 42.47790561\n",
      "Iteration 103, loss = 42.21869831\n",
      "Iteration 104, loss = 42.70713156\n",
      "Iteration 105, loss = 40.95232090\n",
      "Iteration 106, loss = 40.91785846\n",
      "Iteration 107, loss = 41.33473899\n",
      "Iteration 108, loss = 41.48552005\n",
      "Iteration 109, loss = 39.92268025\n",
      "Iteration 110, loss = 39.72146786\n",
      "Iteration 111, loss = 38.85421313\n",
      "Iteration 112, loss = 38.69016798\n",
      "Iteration 113, loss = 38.51524013\n",
      "Iteration 114, loss = 38.23076541\n",
      "Iteration 115, loss = 37.59567641\n",
      "Iteration 116, loss = 37.77011929\n",
      "Iteration 117, loss = 37.31875961\n",
      "Iteration 118, loss = 36.45774216\n",
      "Iteration 119, loss = 36.40532413\n",
      "Iteration 120, loss = 36.66318590\n",
      "Iteration 121, loss = 35.70649204\n",
      "Iteration 122, loss = 35.46801627\n",
      "Iteration 123, loss = 34.85703210\n",
      "Iteration 124, loss = 34.86627349\n",
      "Iteration 125, loss = 34.39088697\n",
      "Iteration 126, loss = 34.58472686\n",
      "Iteration 127, loss = 34.18373054\n",
      "Iteration 128, loss = 33.77928896\n",
      "Iteration 129, loss = 33.34231110\n",
      "Iteration 130, loss = 33.29387597\n",
      "Iteration 131, loss = 33.04284992\n",
      "Iteration 132, loss = 33.14593868\n",
      "Iteration 133, loss = 32.68617234\n",
      "Iteration 134, loss = 32.11848815\n",
      "Iteration 135, loss = 32.59152088\n",
      "Iteration 136, loss = 32.17892016\n",
      "Iteration 137, loss = 31.87891587\n",
      "Iteration 138, loss = 31.71935223\n",
      "Iteration 139, loss = 31.12314981\n",
      "Iteration 140, loss = 31.79212918\n",
      "Iteration 141, loss = 30.49005996\n",
      "Iteration 142, loss = 31.43364461\n",
      "Iteration 143, loss = 31.37886979\n",
      "Iteration 144, loss = 30.55163178\n",
      "Iteration 145, loss = 30.43986266\n",
      "Iteration 146, loss = 30.04159265\n",
      "Iteration 147, loss = 29.68220520\n",
      "Iteration 148, loss = 29.53323335\n",
      "Iteration 149, loss = 29.82792218\n",
      "Iteration 150, loss = 29.90048883\n",
      "Iteration 151, loss = 29.19637603\n",
      "Iteration 152, loss = 29.50096842\n",
      "Iteration 153, loss = 28.99053205\n",
      "Iteration 154, loss = 28.92077642\n",
      "Iteration 155, loss = 28.66158056\n",
      "Iteration 156, loss = 28.00650534\n",
      "Iteration 157, loss = 28.38965096\n",
      "Iteration 158, loss = 28.36604000\n",
      "Iteration 159, loss = 28.03657346\n",
      "Iteration 160, loss = 28.07866063\n",
      "Iteration 161, loss = 27.94572474\n",
      "Iteration 162, loss = 27.59306160\n",
      "Iteration 163, loss = 27.83825201\n",
      "Iteration 164, loss = 27.73411228\n",
      "Iteration 165, loss = 27.11841203\n",
      "Iteration 166, loss = 27.06265726\n",
      "Iteration 167, loss = 27.41030717\n",
      "Iteration 168, loss = 27.59605171\n",
      "Iteration 169, loss = 26.86588690\n",
      "Iteration 170, loss = 27.35164578\n",
      "Iteration 171, loss = 26.28787004\n",
      "Iteration 172, loss = 26.30118541\n",
      "Iteration 173, loss = 26.41842048\n",
      "Iteration 174, loss = 26.60876151\n",
      "Iteration 175, loss = 26.17155880\n",
      "Iteration 176, loss = 26.10543881\n",
      "Iteration 177, loss = 26.30656021\n",
      "Iteration 178, loss = 25.74693690\n",
      "Iteration 179, loss = 26.03811856\n",
      "Iteration 180, loss = 25.70543535\n",
      "Iteration 181, loss = 25.35629255\n",
      "Iteration 182, loss = 25.59566092\n",
      "Iteration 183, loss = 25.42112570\n",
      "Iteration 184, loss = 25.84942055\n",
      "Iteration 185, loss = 24.74325068\n",
      "Iteration 186, loss = 25.05397568\n",
      "Iteration 187, loss = 25.02030103\n",
      "Iteration 188, loss = 25.06478815\n",
      "Iteration 189, loss = 25.17580169\n",
      "Iteration 190, loss = 25.15501785\n",
      "Iteration 191, loss = 24.46505593\n",
      "Iteration 192, loss = 24.88920474\n",
      "Iteration 193, loss = 24.62438032\n",
      "Iteration 194, loss = 24.27614180\n",
      "Iteration 195, loss = 24.42107908\n",
      "Iteration 196, loss = 24.52102958\n",
      "Iteration 197, loss = 24.22798486\n",
      "Iteration 198, loss = 23.57393531\n",
      "Iteration 199, loss = 24.17450302\n",
      "Iteration 200, loss = 24.47271804\n",
      "Iteration 201, loss = 24.04525038\n",
      "Iteration 202, loss = 23.66536319\n",
      "Iteration 203, loss = 23.75898115\n",
      "Iteration 204, loss = 23.82622978\n",
      "Iteration 205, loss = 24.27979783\n",
      "Iteration 206, loss = 23.31855695\n",
      "Iteration 207, loss = 23.39258819\n",
      "Iteration 208, loss = 23.39044140\n",
      "Iteration 209, loss = 23.65724833\n",
      "Iteration 210, loss = 23.38755510\n",
      "Iteration 211, loss = 23.04774648\n",
      "Iteration 212, loss = 23.55195659\n",
      "Iteration 213, loss = 22.85032037\n",
      "Iteration 214, loss = 22.61504300\n",
      "Iteration 215, loss = 22.93743914\n",
      "Iteration 216, loss = 22.84862467\n",
      "Iteration 217, loss = 22.68093417\n",
      "Iteration 218, loss = 22.85594542\n",
      "Iteration 219, loss = 22.74687111\n",
      "Iteration 220, loss = 23.18462025\n",
      "Iteration 221, loss = 22.40975204\n",
      "Iteration 222, loss = 22.54062666\n",
      "Iteration 223, loss = 22.19903156\n",
      "Iteration 224, loss = 22.50639010\n",
      "Iteration 225, loss = 22.05091948\n",
      "Iteration 226, loss = 21.95211470\n",
      "Iteration 227, loss = 22.67397989\n",
      "Iteration 228, loss = 22.80911150\n",
      "Iteration 229, loss = 21.95874796\n",
      "Iteration 230, loss = 21.68475839\n",
      "Iteration 231, loss = 22.09540366\n",
      "Iteration 232, loss = 22.55877809\n",
      "Iteration 233, loss = 22.06204681\n",
      "Iteration 234, loss = 21.80507189\n",
      "Iteration 235, loss = 21.73985973\n",
      "Iteration 236, loss = 21.51920247\n",
      "Iteration 237, loss = 21.98796477\n",
      "Iteration 238, loss = 21.80202415\n",
      "Iteration 239, loss = 21.32930768\n",
      "Iteration 240, loss = 21.55664080\n",
      "Iteration 241, loss = 21.72881528\n",
      "Iteration 242, loss = 21.68417623\n",
      "Iteration 243, loss = 21.64374422\n",
      "Iteration 244, loss = 21.30273590\n",
      "Iteration 245, loss = 20.94322991\n",
      "Iteration 246, loss = 21.13637057\n",
      "Iteration 247, loss = 20.98206998\n",
      "Iteration 248, loss = 21.24095554\n",
      "Iteration 249, loss = 21.14563052\n",
      "Iteration 250, loss = 20.57902788\n",
      "Iteration 251, loss = 21.48033258\n",
      "Iteration 252, loss = 20.86860845\n",
      "Iteration 253, loss = 20.83995506\n",
      "Iteration 254, loss = 21.42398981\n",
      "Iteration 255, loss = 20.70912425\n",
      "Iteration 256, loss = 20.84418968\n",
      "Iteration 257, loss = 20.46430638\n",
      "Iteration 258, loss = 20.56067556\n",
      "Iteration 259, loss = 21.05126795\n",
      "Iteration 260, loss = 20.95317395\n",
      "Iteration 261, loss = 20.41517360\n",
      "Iteration 262, loss = 20.45012412\n",
      "Iteration 263, loss = 20.40592672\n",
      "Iteration 264, loss = 20.65824511\n",
      "Iteration 265, loss = 20.27261492\n",
      "Iteration 266, loss = 20.50866668\n",
      "Iteration 267, loss = 20.45582974\n",
      "Iteration 268, loss = 20.58206323\n",
      "Iteration 269, loss = 20.02278280\n",
      "Iteration 270, loss = 20.33247164\n",
      "Iteration 271, loss = 20.93568513\n",
      "Iteration 272, loss = 20.15076001\n",
      "Iteration 273, loss = 20.39889432\n",
      "Iteration 274, loss = 20.01480494\n",
      "Iteration 275, loss = 20.07822990\n",
      "Iteration 276, loss = 19.89860426\n",
      "Iteration 277, loss = 19.84101296\n",
      "Iteration 278, loss = 19.74902839\n",
      "Iteration 279, loss = 20.00590377\n",
      "Iteration 280, loss = 19.96308655\n",
      "Iteration 281, loss = 19.93235305\n",
      "Iteration 282, loss = 19.82591498\n",
      "Iteration 283, loss = 19.72128098\n",
      "Iteration 284, loss = 19.99313863\n",
      "Iteration 285, loss = 19.45498227\n",
      "Iteration 286, loss = 19.54408404\n",
      "Iteration 287, loss = 20.08957385\n",
      "Iteration 288, loss = 19.73227424\n",
      "Iteration 289, loss = 19.37876481\n",
      "Iteration 290, loss = 19.77059069\n",
      "Iteration 291, loss = 19.80499426\n",
      "Iteration 292, loss = 19.34707519\n",
      "Iteration 293, loss = 19.49219616\n",
      "Iteration 294, loss = 19.58222436\n",
      "Iteration 295, loss = 19.29887111\n",
      "Iteration 296, loss = 19.71190693\n",
      "Iteration 297, loss = 19.36274375\n",
      "Iteration 298, loss = 19.41965682\n",
      "Iteration 299, loss = 19.23092583\n",
      "Iteration 300, loss = 19.23350828\n",
      "Iteration 301, loss = 19.24014182\n",
      "Iteration 302, loss = 19.05998483\n",
      "Iteration 303, loss = 19.78212996\n",
      "Iteration 304, loss = 19.01991425\n",
      "Iteration 305, loss = 19.36948816\n",
      "Iteration 306, loss = 19.34253596\n",
      "Iteration 307, loss = 18.69522660\n",
      "Iteration 308, loss = 18.80903503\n",
      "Iteration 309, loss = 18.89157513\n",
      "Iteration 310, loss = 19.00961090\n",
      "Iteration 311, loss = 19.01871639\n",
      "Iteration 312, loss = 18.87495899\n",
      "Iteration 313, loss = 18.80787674\n",
      "Iteration 314, loss = 18.84050767\n",
      "Iteration 315, loss = 19.14720888\n",
      "Iteration 316, loss = 18.76896057\n",
      "Iteration 317, loss = 18.47217589\n",
      "Iteration 318, loss = 19.04256161\n",
      "Iteration 319, loss = 18.49977471\n",
      "Iteration 320, loss = 19.00864784\n",
      "Iteration 321, loss = 18.74527681\n",
      "Iteration 322, loss = 18.65515436\n",
      "Iteration 323, loss = 18.42873071\n",
      "Iteration 324, loss = 18.42278754\n",
      "Iteration 325, loss = 18.83895775\n",
      "Iteration 326, loss = 18.34122951\n",
      "Iteration 327, loss = 18.33309893\n",
      "Iteration 328, loss = 18.11630614\n",
      "Iteration 329, loss = 18.67337570\n",
      "Iteration 330, loss = 18.74608477\n",
      "Iteration 331, loss = 18.20373247\n",
      "Iteration 332, loss = 18.12951759\n",
      "Iteration 333, loss = 18.22419503\n",
      "Iteration 334, loss = 18.66089212\n",
      "Iteration 335, loss = 18.11715722\n",
      "Iteration 336, loss = 18.42567472\n",
      "Iteration 337, loss = 18.27638896\n",
      "Iteration 338, loss = 18.10878624\n",
      "Iteration 339, loss = 18.11921111\n",
      "Iteration 340, loss = 18.59227125\n",
      "Iteration 341, loss = 17.79556579\n",
      "Iteration 342, loss = 18.06685929\n",
      "Iteration 343, loss = 17.93517388\n",
      "Iteration 344, loss = 18.32368189\n",
      "Iteration 345, loss = 17.89119935\n",
      "Iteration 346, loss = 18.05227925\n",
      "Iteration 347, loss = 17.76616460\n",
      "Iteration 348, loss = 17.96809865\n",
      "Iteration 349, loss = 17.69675583\n",
      "Iteration 350, loss = 17.96409446\n",
      "Iteration 351, loss = 17.85644493\n",
      "Iteration 352, loss = 18.14595822\n",
      "Iteration 353, loss = 17.69784228\n",
      "Iteration 354, loss = 17.55801108\n",
      "Iteration 355, loss = 18.43073008\n",
      "Iteration 356, loss = 17.74784037\n",
      "Iteration 357, loss = 17.19099666\n",
      "Iteration 358, loss = 18.01447767\n",
      "Iteration 359, loss = 17.79635146\n",
      "Iteration 360, loss = 17.15182044\n",
      "Iteration 361, loss = 17.91061267\n",
      "Iteration 362, loss = 17.58710338\n",
      "Iteration 363, loss = 18.09036954\n",
      "Iteration 364, loss = 17.37733767\n",
      "Iteration 365, loss = 17.27528488\n",
      "Iteration 366, loss = 17.50851651\n",
      "Iteration 367, loss = 17.55393072\n",
      "Iteration 368, loss = 17.34177204\n",
      "Iteration 369, loss = 17.36817178\n",
      "Iteration 370, loss = 17.37060636\n",
      "Iteration 371, loss = 17.77602076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Mean squared error: 0.942152747065974\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = reg.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'../models/model-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.pkl'\n",
    "os.mknod(path)\n",
    "\n",
    "with open(path, 'ab') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '../models/model-20240109150324.pkl'\n",
    "reg: MLPRegressor\n",
    "with open(load_path, 'rb') as file:\n",
    "    reg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimation(reg, fen):\n",
    "    board = chess.Board(fen)\n",
    "    X = bitarray_to_ndarray(BoardEncoder.encode(board)).reshape(1, -1)\n",
    "    return reg.predict(X)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/8/8/8/3k4/8/3K4/3Q4_w_-_-_0_1?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.3298840046165"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '8/8/8/8/3k4/8/3K4/3Q4 w - - 0 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/k7/p7/8/K7/8/8/8/8_w_-_-_0_1?color=white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/4k3/8/4K3/4P3/8/8/8/8_w_-_-_0_1?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.351667841368029"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '4k3/8/4K3/4P3/8/8/8/8 w - - 0 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/8/4k3/4P3/4K3/8/8/8_w_-_-_0_1?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.41495722508689"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '8/8/4k3/4P3/4K3/8/8/8 w - - 0 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/4P3/4K3/8/8/8/8/7k_w_-_-_0_1?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.62148691642241"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '8/4P3/4K3/8/8/8/8/7k w - - 0 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White to move:\n",
    "\n",
    "![](https://fen2image.chessvision.ai/8/8/8/7K/8/2k5/4P3/8_b_-_-_11_6?color=white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.71839235947378"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '8/8/8/7K/8/2k5/4P3/8 w - - 11 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.70304034337242"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(reg, '8/3Q4/k7/8/8/8/1K6/8 w - - 5 27')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/model-20231218192512.pkl', 'rb') as file:\n",
    "    model_KQ_K = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.24994928207494"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimation(model_KQ_K, '8/3Q4/k7/8/8/8/1K6/8 w - - 5 27')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
